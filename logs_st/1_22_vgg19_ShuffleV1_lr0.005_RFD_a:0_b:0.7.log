==> loading teacher model
==> done
Test: [0/750]	Time 24.384 (24.384)	Loss 0.5613 (0.5613)	Acc@1 81.250 (81.250)	Acc@5 93.750 (93.750)
Test: [100/750]	Time 0.047 (0.291)	Loss 0.6912 (0.4886)	Acc@1 75.000 (87.098)	Acc@5 100.000 (95.235)
Test: [200/750]	Time 0.033 (0.168)	Loss 1.2877 (0.4868)	Acc@1 50.000 (85.106)	Acc@5 93.750 (96.253)
Test: [300/750]	Time 0.050 (0.125)	Loss 1.1032 (0.6946)	Acc@1 56.250 (76.412)	Acc@5 93.750 (95.515)
Test: [400/750]	Time 0.042 (0.106)	Loss 0.5579 (0.7766)	Acc@1 84.375 (73.208)	Acc@5 93.750 (94.966)
Test: [500/750]	Time 0.040 (0.093)	Loss 0.4078 (0.7378)	Acc@1 84.375 (75.087)	Acc@5 100.000 (94.966)
Test: [600/750]	Time 0.032 (0.083)	Loss 0.5665 (0.7392)	Acc@1 71.875 (75.151)	Acc@5 96.875 (94.982)
Test: [700/750]	Time 0.033 (0.077)	Loss 1.0252 (0.7310)	Acc@1 68.750 (75.334)	Acc@5 81.250 (95.136)
 * Acc@1 75.533 Acc@5 95.092
teacher accuracy:  tensor(75.5333, device='cuda:0')
==> training...
Epoch: [1][0/875]	Time 3.374 (3.374)	Data 1.451 (1.451)	Loss 28.5286 (28.5286)	Loss@kd 36.9869 (36.9869)	Acc@1 14.062 (14.062)	Acc@5 64.062 (64.062)
Epoch: [1][100/875]	Time 0.639 (0.684)	Data 0.007 (0.021)	Loss 5.6210 (31.8134)	Loss@kd 5.6151 (11.5404)	Acc@1 40.625 (27.212)	Acc@5 95.312 (77.367)
Epoch: [1][200/875]	Time 0.645 (0.670)	Data 0.006 (0.014)	Loss 4.4155 (18.6217)	Loss@kd 3.9224 (8.1320)	Acc@1 34.375 (31.670)	Acc@5 93.750 (82.128)
